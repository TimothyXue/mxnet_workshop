{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Each of the code cells can be run by selecting that cell and pressing `SHIFT+ENTER`. To restart the notebook, you can select in the menu above Kernel -> Restart & Clear Output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Movie Reviews\n",
    "\n",
    "\n",
    "In this exercise, we will write a model to analyze movie reviews on IMDB and decide if they are positive or negative reviews.\n",
    "\n",
    "The IMDB dataset consists of 25,000 reviews, each with a binary label (1 = positive, 0 = negative).\n",
    "\n",
    "Here is an example of a POSITIVE review:\n",
    "\n",
    "> \"The pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. Naturally Joyce's short story lends the film a ready made structure as perfect as a polished diamond, but the small changes Huston makes such as the inclusion of the poem fit in neatly. It is truly a masterpiece of tact, subtlety and overwhelming beauty.\"\n",
    "\n",
    "Here is an example of a NEGATIVE review:\n",
    "\n",
    "> \"Beautiful attracts excellent idea, but ruined with a bad selection of the actors. The main character is a loser and his woman friend and his friend upset viewers. Apart from the first episode all the other become more boring and boring. First, it considers it illogical behavior. No one normal would not behave the way the main character behaves. It all represents a typical Halmark way to endear viewers to the reduced amount of intelligence. Does such a scenario, or the casting director and destroy this question is on Halmark producers. Cat is the main character is wonderful. The main character behaves according to his friend selfish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup\n",
    "--------\n",
    "\n",
    "We first import the packages we need to run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "import imdb as IMDB\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dataset\n",
    "----------\n",
    "\n",
    "We have to preprocess the dataset to convert the words into numbers. We take our vocabularly of words, and assign a number to each word. For example, a sentence such as:\n",
    "\n",
    "> \"Hello world, my name is Intel and my location is Santa Clara\"\n",
    "\n",
    "Will be converted to list of 6 numbers:\n",
    "\n",
    "> [24, 784, 4, 98, 22, 143, 15, 4, 314, 22, 488, 2894] \n",
    "\n",
    "We already done this for you, and is loaded in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb = IMDB.IMDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build the Model\n",
    "-----------------\n",
    "\n",
    "The network consists of list of the following layers:\n",
    "\n",
    "1. `Embedding` transforms each word into a vector of numbers. \n",
    "2. `LSTMCell` is a recurrent layer with “long short-term memory” units. LSTM networks are good at learning temporal dependencies in the data.\n",
    "3. `sum` sums the activations over the time\n",
    "3. `Dropout` randomly silences a subset of the units during training.\n",
    "4. `FullyConnected` is a layer with two outputs, for the two target classes.\n",
    "\n",
    "Below we construct the graph of operations by first creating placeholders for the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mx.sym.Variable('data')\n",
    "label = mx.sym.Variable('softmax_label')\n",
    "label = mx.sym.Reshape(data=label, shape=(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate our graph by passing the data through the layers of the network, starting with the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = mx.sym.Embedding(data=data, input_dim=20000, output_dim=128, name='embed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LSTM layer, we unroll the layer over time, then pass the outputs to the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_cell = mx.rnn.LSTMCell(num_hidden=64)\n",
    "net, _ = lstm_cell.unroll(length=128, inputs=net, merge_outputs=True)\n",
    "net = mx.sym.sum(data=net, axis=1)\n",
    "net = mx.sym.Dropout(data=net, p=0.5)\n",
    "net = mx.sym.FullyConnected(data=net, num_hidden=2)\n",
    "net = mx.sym.SoftmaxOutput(data=net, label=label, name='softmax')\n",
    "\n",
    "init = mx.init.Mixed(patterns=['embed', '.*'], \n",
    "                     initializers= [mx.init.Uniform(scale=1/128), mx.init.Xavier()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the Module API, which providers helper functions to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.mod.Module(net, context=mx.cpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks allow the model to report its progress during the course of training. Here we tell MXNET to plot a graph with the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VisCostCallback import CostVisCallback\n",
    "\n",
    "callbacks = CostVisCallback(nepochs=2.0, y_range=(0, 4.5), total_batches=156).get_callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Train the model.\n",
    "------------\n",
    "Now are ready to train the model. Recall what happens during the training process:\n",
    "\n",
    "<img src=\"images/train_schematic.png\", width=700px>\n",
    "\n",
    "To train the model, we call the `fit()` function and pass in the training set, and other settings. Here we train for 2 epochs, meaning two rounds through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        train_data          = imdb.train_set,\n",
    "        optimizer           = 'Adagrad',\n",
    "        eval_metric         = mx.metric.CrossEntropy(),\n",
    "        optimizer_params    = {'learning_rate': 0.01},\n",
    "        initializer         = init,\n",
    "        num_epoch           = 2,\n",
    "        batch_end_callback  = callbacks['train_cost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "--------\n",
    "\n",
    "We can then measure the model's accuracy on the validation data -- data that the model was not trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(imdb.valid_set, mx.metric.Accuracy())\n",
    "print \"Test  Accuracy - {}\".format(100 * score[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference\n",
    "--------\n",
    "\n",
    "Now let's do something fun with the trained model! We create a UI below where you can type in your movie review (or any other text) and have it classified into positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imdb import preprocess, text_window\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "def inference(x):\n",
    "    inputs = preprocess(x)\n",
    "    output = model.predict(eval_data=inputs).asnumpy()\n",
    "    score = output[0][1]\n",
    "    print(\"Sentiment: {:.1f}% Positive\".format(100*score))\n",
    "\n",
    "z = interact(inference, x=text_window())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\"This movie was great!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
